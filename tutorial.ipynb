{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json \n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "from lc_macro_pipeline.retiler import Retiler\n",
    "from lc_macro_pipeline.data_processing import DataProcessing\n",
    "from lc_macro_pipeline.geotiff_writer import Geotiff_writer\n",
    "from lc_macro_pipeline.macro_pipeline import MacroPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Macro-ecology LiDAR point-cloud processing pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Retrieval  and Cluster Setup\n",
    "\n",
    "Files produced by the pipeline will be saved in the `temp_folder` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_folder = Path('/var/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by checking whether the test data set is available locally, we otherwise retrieve it from the AHN3 repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_files = ['C_41CZ2.LAZ']\n",
    "\n",
    "file_paths = [temp_folder.joinpath(f) for f in testdata_files]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    if not file_path.is_file():\n",
    "        file_url = '/'.join(['https://geodata.nationaalgeoregister.nl/ahn3/extract/ahn3_laz', \n",
    "                             file_path.name])\n",
    "        urlretrieve(file_url, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then setup the cluster that we will use for the computation using `dask`. For this example, the cluster consists of 3 processes. Note: it is important that single-threaded workers are employed for the tasks that require `laserchicken`!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(processes=True, \n",
    "                       n_workers=3, \n",
    "                       threads_per_worker=1, \n",
    "                       local_directory=temp_folder.joinpath('dask-worker-space'))\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retiling\n",
    "\n",
    "The first step in the pipeline is to retile the retrieved point-cloud files to a regular grid, splitting the original data into smaller chuncks that are easier to handle for data processing. The boundaries of the grid and the number of tiles along each axis are set to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    'min_x': -113107.8100,\n",
    "    'max_x': 398892.1900,\n",
    "    'min_y': 214783.8700,\n",
    "    'max_y': 726783.87,\n",
    "    'n_tiles_side': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retiling of multiple input files consists of independent tasks, which are thus efficiently parallelized. The input controlling all the steps of the retiling is organized in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retiling_macro = MacroPipeline()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(file_path)\n",
    "    \n",
    "    retiler = Retiler()\n",
    "    \n",
    "    retiler.input = {\n",
    "        'localfs': {\n",
    "            'input_folder': file_path.parent.as_posix(),\n",
    "            'input_file': file_path.name,\n",
    "            'temp_folder': temp_folder\n",
    "        },\n",
    "        'tiling': grid,\n",
    "        'split_and_redistribute': {},\n",
    "        'validate': {}\n",
    "    }\n",
    "    \n",
    "    retiling_macro.add_task(retiler)\n",
    "\n",
    "res = retiling_macro.run(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction\n",
    "\n",
    "Once the files are splitted into tiles of a manageable size, we proceed to the feature extraction stage, which is performed using `laserchicken`. We choose the following two example features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"mean_normalized_height\", \"std_normalized_height\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base input dictionary for this step looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_input = {\n",
    "    \"normalize\": {\n",
    "        \"cell_size\": 1\n",
    "    },\n",
    "    \"generate_targets\": {\n",
    "        'min_x': -113107.8100,\n",
    "        'max_x': 398892.1900,\n",
    "        'min_y': 214783.8700,\n",
    "        'max_y': 726783.87,\n",
    "        'n_tiles_side': 256,\n",
    "        \"tile_mesh_size\" : 10.0,\n",
    "        \"validate\" : True,\n",
    "    },\n",
    "    \"extract_features\": {\n",
    "        \"feature_names\": feature_names,\n",
    "        \"volume_type\": \"cell\",\n",
    "        \"volume_size\": 10\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tiles to which the original input file has been retiled are listed in a record file located in the temporary directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = []\n",
    "for file_path in file_paths:\n",
    "    record_file = '_'.join([file_path.stem, 'retile_record.js'])\n",
    "    with Path(temp_folder/file_path.stem/record_file).open() as f:\n",
    "        record = json.load(f)\n",
    "    assert record['validated']\n",
    "    tiles += [Path(temp_folder/file_path.stem/tile)\n",
    "              for tile in record['redistributed_to']]\n",
    "print([t.as_posix() for t in tiles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tile can be processed independently, so that again one can run the tasks in a parallel fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_macro = MacroPipeline()\n",
    "\n",
    "for tile in tiles:\n",
    "    print(tile)\n",
    "    \n",
    "    dp = DataProcessing()\n",
    "\n",
    "    # add tile-specific input to the dictionary\n",
    "    dp_input['load'] = {'path': tile.as_posix()}\n",
    "    dp_input['export_targets'] = {'path': tile.with_suffix('.ply').as_posix(), 'overwrite': True}\n",
    "    dp_input['generate_targets']['index_tile_x'] = int(tile.name.split('_')[1]) \n",
    "    dp_input['generate_targets']['index_tile_y'] = int(tile.name.split('_')[2])\n",
    "    dp.input = copy.deepcopy(dp_input)\n",
    "    \n",
    "    dp_macro.add_task(dp)\n",
    "    \n",
    "res = dp_macro.run(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GeoTIFF Export\n",
    "\n",
    "The last step of the pipeline is the transformation of the features extracted from the point-cloud data and 'rasterized' in the target grid to a GeoTIFF file. We set the path where input will be read and output written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(temp_folder/file_paths[0].stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the construction of the geotiffs (one per feature) can be performed in parallel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotiff_macro = MacroPipeline()\n",
    "\n",
    "for feature_name in feature_names:\n",
    "    print(feature_name)\n",
    "    \n",
    "    gw = Geotiff_writer()\n",
    "\n",
    "    gw.input = {\n",
    "        \"parse_point_cloud\" : {\"data_directory\": path.as_posix()},\n",
    "        \"data_split\": {\"xSub\": 1, \"ySub\": 1},\n",
    "        \"create_subregion_geotiffs\": {\n",
    "            \"outputdir\": path.as_posix(), \n",
    "            \"outputhandle\": \"geotiff\",\n",
    "            \"band_export\": [feature_name]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    geotiff_macro.add_task(gw)\n",
    "\n",
    "res = geotiff_macro.run(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we stop the client and the scheduler of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
